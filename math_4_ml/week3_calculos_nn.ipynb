{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week3_calculos_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTIxBX/IKwIEHXD/nVWzEh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geocarvalho/python-ds/blob/main/math_4_ml/week3_calculos_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVG074OSqcRo"
      },
      "source": [
        "# Simple Artificial Neural Networks\n",
        "\n",
        "1. Uma rede neural simples:\n",
        "\n",
        "![nn](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/JKIDlfihEeeAPQprC3K2Bg_66921c651e175da7cc8b805860da4225_simple.png?expiry=1623024000000&hmac=SRXuFKpl75w2u_LFt6LI9UvE1ZTimtrcg_nGBZc8tI0)\n",
        "\n",
        "* Aqui temos apenas dois neuronios (ou nos) e eles estao sendo ligados por apenas uma aresta.\n",
        "\n",
        "* A ativacao dos neuronios na ultima camada (1) e determinada pela ativacao de neuronios na camada previa (0).\n",
        "\n",
        "$$a^{(1)} = \\sigma(w^{(1)}a^{(0)} + b^{(1)})$$\n",
        "\n",
        "Onde $w^{(1)}$ e o peso das conexoes entre neuronio (0) e neuronio (1). Ja $b^{(1)}$ e o vies (bias) do neuronio (1). Esses sao entao sujeitos a funcao de ativacao ($\\sigma$) para a ativacao do neuronio (1).\n",
        "\n",
        "* Nossa pequena rede neural nao sera capaz de muita coisa por ser muito simples, mas e interessante por alguns numeros de entrada para sentir como as coisas funcionam.\n",
        "\n",
        "* Vamos assumir que queremos treinar a rede para dar uma funcao negativa, ou seja, quando pomos 1 ela retorna 0 e quando botamos 0 ela retorn 1.\n",
        "\n",
        "* Para simplicidade, vamos usar $\\sigma(z) = tanh(z)$ para nossa funcao de ativacao e iniciar de forma aleatoria nossos pesos e vieses com $w^{(1)} = 1.3$ e $b^{(1)} = -0.1$.\n",
        "\n",
        "* Abaixo temos o codigo para ver quais valores de saida a rede neural retorna inicialmente para os dados de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B4gRqnOqOj5",
        "outputId": "0598221f-1e8f-4c30-f2ad-afe587a6ef67"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Primeiro organizamos as variaveis\n",
        "sigma = np.tanh\n",
        "w1 = 1.3\n",
        "b1 = -0.1\n",
        "\n",
        "# Entao definimos a funcao de ativacao do neuronio\n",
        "def a1(a0):\n",
        "  return sigma(w1 * a0 + b1)\n",
        "\n",
        "# Agora podemos testar a rede\n",
        "print(a1(1))\n",
        "print(a1(0))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8336546070121552\n",
            "-0.09966799462495582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCvUPhwWt3yr"
      },
      "source": [
        "* Nao e tao boa, mas nao foi treinada ainda. Vamos experimentar mudar os valores de peso e vies e ver no que da."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4GbQcE6twyM",
        "outputId": "e3dd0b30-260a-43cd-d4a2-f9821a73843d"
      },
      "source": [
        "w1 = 0\n",
        "b1 = 5\n",
        "\n",
        "print(a1(1))\n",
        "print(a1(0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9999092042625951\n",
            "0.9999092042625951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69lY8lv2uMGt",
        "outputId": "ab5ffe69-b9ad-47e0-b56c-578b64b8c164"
      },
      "source": [
        "w1 = -10\n",
        "b1 = 10\n",
        "\n",
        "print(a1(1))\n",
        "print(a1(0))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.9999999958776927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSuVGD-Uulrf"
      },
      "source": [
        "2. Agora vamos estender nossa rede neural simples para ter mais neuronios.\n",
        "\n",
        "![nn2](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/BmRF4RgTEei0zRLQ2V54Gg_f661933d95252c6e3d84e779bde7c119_moreNodes.png?expiry=1623024000000&hmac=kDXAJKXz0MyiyeElw6-Oy9jGvRktO0ybCRZh031kt2I)\n",
        "\n",
        "* Agora nos mudamos ligeiramente as anotacoes. Os neuronios estao classificados de acordo com sua camada e sobrescrito com seu numero na camada, formando vetors $\\mathbf{a}^{(0)}$ e $\\mathbf{a}^{(1)}$.\n",
        "\n",
        "* Os pesos agora formam uma matriz $\\mathbf{W}^{(1)}$, onde cada elemento, $w^{(1)}_{ij}$ e a combinacao entre o neuronio $j$ na camada anterior e o neuronio $i$ na camada atual.\n",
        "\n",
        "> Por exemplo, $w^{(1)}_{12}$ esta vinculando $a^{(0)}_{2}$ ao $a^{(1)}_{1}$.\n",
        "\n",
        "* Os vieses formam um vetor $\\mathbf{b}^{(1)}$ da mesma forma. Entao podemos atualizar a funcao de ativacao:\n",
        "\n",
        "$$\\mathbf{a}^{(1)} = \\sigma(\\mathbf{W}^{(1)}\\mathbf{a}^{(0)}+\\mathbf{b}^{(1)})$$\n",
        "\n",
        "* Onde todas as variaveis de interesse foram transformadas em vetores ou matrizes e $\\sigma$ age em cada elemento do vetor da soma dos pesos separadamente. \n",
        "\n",
        "* Vamos calcular $\\mathbf{a}^{(1)}$, dado os valores das variaveis:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm6wQr0buP0y",
        "outputId": "aa82f71a-23f9-40f8-80a1-bb704965f7bd"
      },
      "source": [
        "# Iniciando as variaveis\n",
        "sigma = np.tanh\n",
        "W = np.array([[-2, 4, -1], [6, 0, -3]])\n",
        "b = np.array([0.1, -2.5])\n",
        "x = np.array([0.3, 0.4, 0.1])\n",
        "\n",
        "a1 = sigma(np.dot(W, x)+b)\n",
        "print(a1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.76159416 -0.76159416]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3nG6e2X2YGs"
      },
      "source": [
        "3. Agora vamos dar uma olhada numa rede com camada escondida.\n",
        "\n",
        "![nn3](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/lO8V1vlhEeetIhICvTHYsg_3953370b0690ae02c046d9a903540998_moreBoth.png?expiry=1623024000000&hmac=jYFJVrOqfTcmffScbv-57VUEFoQhFH3UnOIUgxyLN-g)\n",
        "\n",
        "* Os dados de entrada estao na camada (0), isso ativa os neuronios da camada (1) que criam os dados de entrada dos neuronios da camada (2).\n",
        "\n",
        "* O numero de pesos na camada sera o produto do numero de neuronios de entrada e de saida na camada. Nesse cada temos 12 pesos na primeira camada e 6 pesos na segunda.\n",
        "\n",
        "* Essa rede tem 5 vieses, no geral existem tantos vieses quanto neuronios de saide ou camada escondida.\n",
        "\n",
        "$$\\mathbf{a}^{(2)} = \\sigma(\\mathbf{W}^{(2)} \\sigma(\\mathbf{W}^{(1)}\\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}) + \\mathbf{b}^{(2)})$$\n",
        "\n",
        "> Nessa forma a cima vemos a funcao inteira para a rede neural com as camadas ligadas. Sendo a mesma coisa que:\n",
        "\n",
        "$$\\mathbf{a}^{(2)} = \\sigma(\\mathbf{W}^{(2)}\\mathbf{a}^{(1)} + \\mathbf{b}^{(2)})$$\n",
        "\n",
        "$$\\mathbf{a}^{(1)} = \\sigma(\\mathbf{W}^{(1)}\\mathbf{a}^{(0)} + \\mathbf{b}^{(1)})$$\n",
        "\n",
        "--- \n",
        "\n",
        "# Training Neural Networks\n",
        "\n",
        "Agora vamos voltar a nossa rede neural simples para ver mais detalhes sobre *back-propagation* usando regra da cadeia para treinar nossa rede neural.\n",
        "\n",
        "![nn00](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/f1Sg0vr6EeeuoQ4MK7iWlg_0cac092b955dd9547d8cf285c7acc6a4_simple.png?expiry=1623024000000&hmac=3scBq_mquOuxJ-cYWR5syvJoYO86-k86UPdrmhwKtCc)\n",
        "\n",
        "* Para relembrarmos sobre a equacao de ativacao:\n",
        "\n",
        "$$a^{(1)}=\\sigma(z^{(1)})$$\n",
        "\n",
        "$$z^{(1)}=w^{(1)}a^{(0)}+b^{(1)}$$\n",
        "\n",
        "> Onde esse $z^{(1)}$ e a soma dos pesos da ativacao e dos vieses.\n",
        "\n",
        "* Nos podemos formalizar quao bom (ou ruim) nossa rede neural e pegando o comportamento de interesse. Para uma determinada entrada $x$ e uma saida desajada $y$, podemos definir o **custo** especifico desse **treinamento exemplo** como o quadrado das diferencas entre as saidas da rede e a saida desejada:\n",
        "\n",
        "$$ C_k = (a^{(1)} - y)^2$$\n",
        "\n",
        "> Onde $k$ e o treinamento de exemplo e $a^{(1)}$ e assumido como a ativacao do neuronio de saida quando o neuronio de entrada $a^{(0)}$ e usado $x$.\n",
        "\n",
        "* Vamos ver o detalhe de como aplicar isso em todo o dado de treinamento mais tarde, mas vamos ver um exemplo pequeno agora:\n",
        "\n",
        "Lembra da nossa funcao negativa? Para uma entrada 1 a saida seria 0. Para comecar o peso e vies $w^{(1)}=1.3$ e $b^{(1)}=-0.1$, a rede tem como saida $a^{(1)}=0.834$. Se calcularmos o custo para esse exemplo temos:\n",
        "\n",
        "$$C_1 = (0.834-0)^2 = 0.696$$\n",
        "\n",
        "Vejamos o mesmo calculo para $x=0$ e saida $y=1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2ApzJNA2Tli",
        "outputId": "5d655451-1774-4a87-b310-d16ccd4ad2a9"
      },
      "source": [
        "# Organizando as variaveis\n",
        "sigma = np.tanh\n",
        "w1 = 1.3\n",
        "b1 = -0.1\n",
        "\n",
        "# Definindo o neuronio de ativacao\n",
        "def a1(a0):\n",
        "  z = w1 * a0 + b1\n",
        "  return sigma(z)\n",
        "\n",
        "# Usando outros valores\n",
        "x = 0\n",
        "y = 1\n",
        "a_1 = a1(x)\n",
        "c0 = (a_1 - y)**2\n",
        "c0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.209269698402472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezraBuDRYteN"
      },
      "source": [
        "* Para melhorar a performance de uma rede neural num conjunto de treinamento podemos variar os pesos e os vieses. Conseguimos calcular a derivada do custo do exemplo com respeito a essas quantidades usando a regra da cadeia.\n",
        "\n",
        "$$\\frac{\\partial C_k}{\\partial w^{(1)}}=\\frac{\\partial C_k}{\\partial a^{(1)}}\\frac{\\partial a^{(1)}}{\\partial z^{(1)}}\\frac{\\partial z^{(1)}}{\\partial w^{(1)}}$$\n",
        "\n",
        "* Repetindo as equacoes por coviniencia\n",
        "\n",
        "$$a^{(1)} = \\sigma(z^{(1)})$$\n",
        "\n",
        "$$z^{(1)} = w^{(1)}a^{(0)}+b^{(1)}$$\n",
        "\n",
        "$$C_k = (a^{(1)} - y)**2$$\n",
        "\n",
        "* Individualmente essas derivativas sao simples formulas:\n",
        "\n",
        "$$\\frac{\\partial z^{(1)}}{\\partial b^{(1)}} = 1$$\n",
        "\n",
        "$$\\frac{\\partial C_k}{\\partial a^{(1)}} = 2(a^{(1)}-y)$$\n",
        "\n",
        "$$\\frac{\\partial a^{(1)}}{\\partial z^{(1)}} = \\sigma'(z^{(1)})$$\n",
        "\n",
        "$$\\frac{\\partial z^{(1)}}{\\partial w^{(1)}} = a^{(0)}$$\n",
        "\n",
        "* E a derivativa da `tanh`:\n",
        "\n",
        "$$\\frac{d}{dz}tanh(z) = \\frac{1}{cosh^2z}$$\n",
        "\n",
        "Vamos implementar em codigo agora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56iS1W-1W-I5",
        "outputId": "cb291a06-d7a6-4508-bee5-4c7da6621d3e"
      },
      "source": [
        "# Definir a funcao sigma\n",
        "sigma = np.tanh\n",
        "\n",
        "# Definir a equacao feed-foward\n",
        "def a1(w1, b1, a0):\n",
        "  z = w1 * a0 + b1\n",
        "  return sigma(z)\n",
        "\n",
        "# Definir a equacao de custo\n",
        "def C(w1, b1, x, y):\n",
        "  return (a1(w1, b1, x) - y)**2\n",
        "\n",
        "# Definir a funcao que retorna a derivativa da funcao custo em relacao\n",
        "# aos pesos\n",
        "def dCdw(w1, b1, x, y):\n",
        "  z = w1 * x + b1\n",
        "  # Derivativa do custo com ativacao\n",
        "  dCd1 = 2*(a1(w1, b1, x) - y)\n",
        "  # Derivativa da ativacao com os pesos\n",
        "  dadz = 1/np.cosh(z)**2\n",
        "  # Derivativa da soma dos pesos pelos pesos\n",
        "  dzdw = x\n",
        "  # Retornar a o produto da regra da cadeia\n",
        "  return dCda * dadz * dzdw\n",
        "\n",
        "# Definir a funcao que retorna a derivativa da funcao de custo com respeito\n",
        "# aos vieses\n",
        "def dCdb(w1, b1, x, y):\n",
        "  z = w1 * x + b1\n",
        "  dCda = 2 * (a1(w1, b1, x)- y)\n",
        "  dadz = 1/np.cosh(z)**2\n",
        "  dzdb = 1\n",
        "  return dCda * dadz * dzdb\n",
        "\n",
        "# Testes\n",
        "w1 = 2.3\n",
        "b1 = -1.2\n",
        "x = 0\n",
        "y = 1\n",
        "\n",
        "# Saida de como o custo muda em proporcao a pequenas mudancas no vies\n",
        "print(dCdb(w1, b1, x, y))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.1186026425530913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-0MlZbRjLd0"
      },
      "source": [
        "* Agora com mais neuronios na rede, nossas variaveis viram vetores e matrizes.\n",
        "\n",
        "![n11](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/I3U0sfsYEeegOxLoounLjg_a86ba47587cd1990b57340f837644339_moreNodes.png?expiry=1623024000000&hmac=xZeNgoe_y5Z3blS-Iy-fbCJDj7DYJy4ys2hik9uScn8)\n",
        "\n",
        "$$\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)})$$\n",
        "\n",
        "$$\\mathbf{a}^{(1)} = \\mathbf{W}^{(1)}\\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}$$\n",
        "\n",
        "* A funcao de custo continua sendo um unico valor em vez de virar um vetor, os componentes sao somados para cada neuronio de saida.\n",
        "\n",
        "$$C_k = \\sum_i(a^{(1)}_i - y_i)^2$$\n",
        "\n",
        "> As variaveis com $i$ classificam os neuronios de saida que sao somados, ja $k$ indica os dados de exemplo de treinamento.\n",
        "\n",
        "* Os dados de treinamento se tornam vetores\n",
        "\n",
        "> $x$ $\\to$ $\\mathbf{x}$ e tem o mesmo numero de elementos que neuronios de entrada.\n",
        "\n",
        "> $y$ $\\to$ $\\mathbf{y}$ e tem o mesmo numero de elementos que neuronios de saida.\n",
        "\n",
        "* Isso nos permite escrever a funcao de custo em forma de vetores usando o modulo ao quadrado.\n",
        "\n",
        "$$C_k = |\\mathbf{a}^{(1)}-\\mathbf{y}^2|$$\n",
        "\n",
        "Vamos trabalhar com o codigo agora:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb9oGsk-fIBu",
        "outputId": "7b769370-fe61-4616-cd6b-dd1ac952713f"
      },
      "source": [
        "# Definir a funcao de ativacao\n",
        "sigma = np.tanh\n",
        "\n",
        "# Vamos usar uma inicializacao aleatoria para pesos e vieses\n",
        "W = np.array([[-0.94529712, -0.2667356 , -0.91219181],\n",
        "              [ 2.05529992,  1.21797092,  0.22914497]])\n",
        "b = np.array([ 0.61273249,  1.6422662 ])\n",
        "\n",
        "# Definir a funcao de feed-foward\n",
        "def a1(a0):\n",
        "  # Usar multiplicacao de matrizes\n",
        "  z = W @ a0 + b\n",
        "  return sigma(z)\n",
        "\n",
        "# Nossos dados de treinamento\n",
        "x = np.array([0.1, 0.5, 0.6])\n",
        "y = np.array([0.9, 0.6])#[0.25, 0.75])\n",
        "\n",
        "# Funcao de custo\n",
        "# Diferenca entre vetores de ativacao observada e esperada\n",
        "d = a1(x) - y\n",
        "# Valor absoluto do quadrado das diferencas\n",
        "C = d @ d\n",
        "print(C)\n",
        "\n",
        "# Outro exemplo de entrada\n",
        "x = np.array([0.7, 0.6, 0.2])\n",
        "d = a1(x) - y\n",
        "C = d @ d\n",
        "print(C)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2771038883115056\n",
            "1.7788340952508743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sk7PxAlqkq0"
      },
      "source": [
        "* Vamos falar de uma rede com camada escondida agora.\n",
        "\n",
        "![n22](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/yAAKnP4aEeejaAo3LPV8uA_dcc184247e188ac1d8dc86a4255472aa_moreBoth.png?expiry=1623024000000&hmac=PSot7ksHV2etFT_PyId-KbxB_ASnRZVXYxYg0z8fpdw)\n",
        "\n",
        "* Para treinar essa rede precisamos de **back-propagation**, porque comecamos na camada de saida e calculamos as derivadas de tras para frente ate a camada de entrada usando regra da cadeia.\n",
        "\n",
        "* Se quisessemos calcular a derivada do custo em respeito aos pesos da ultima camada:\n",
        "\n",
        "$$\\frac{\\partial C_k}{\\partial \\mathbf{W}^{(2)}}=\\frac{\\partial C_k}{\\partial \\mathbf{a}^{(2)}}\\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{W}^{(2)}}$$\n",
        "\n",
        "* Tambem podemos construir algo parecido para os vieses.\n",
        "\n",
        "* Se quisermos calcular a derivada do custo em respeito aos pesos da camada anterior:\n",
        "\n",
        "$$\\frac{\\partial C_k}{\\partial \\mathbf{W}^{(1)}}=\\frac{\\partial C_k}{\\partial \\mathbf{a}^{(2)}}\\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{W}^{(1)}}$$\n",
        "\n",
        "> Onde $\\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}$ pode ser expandido para:\n",
        "\n",
        "$$\\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}} = \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{a}^{(1)}}$$\n",
        "\n",
        "> Isso pode ser generalizado para qualquer camada:\n",
        "\n",
        "$$\\frac{\\partial C_k}{\\partial \\mathbf{W}^{(i)}}=\\frac{\\partial C_k}{\\partial \\mathbf{a}^{(N)}}\\frac{\\partial \\mathbf{a}^{(N)}}{\\partial \\mathbf{a}^{(N-1)}}\\frac{\\partial \\mathbf{a}^{(N-1)}}{\\partial \\mathbf{a}^{(N-2)}}\\dots\\frac{\\partial \\mathbf{a}^{(i+1)}}{\\partial \\mathbf{a}^{(i)}}\\frac{\\partial \\mathbf{a}^{(i)}}{\\partial \\mathbf{z}^{(i)}}\\frac{\\partial \\mathbf{z}^{(i)}}{\\partial \\mathbf{W}^{(i)}}$$\n",
        "\n",
        "* Um exemplo de derivativa, para a expressao $\\frac{\\partial \\mathbf{a}^{(j)}}{\\partial \\mathbf{a}^{(j-1)}}$ e as equacoes de ativacao:\n",
        "\n",
        "$$a^{(n)} = \\sigma(z^{(n)})$$\n",
        "\n",
        "$$z^{(n)} = w^{(n)}a^{(n-1)}+b^{(n)}$$\n",
        "\n",
        "> A derivada parcial e $\\sigma'(\\mathbf{z}^{(j)})\\mathbf{W}^{(j)}$\n",
        "\n",
        "# Backpropagation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux_H-a1om_9t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}